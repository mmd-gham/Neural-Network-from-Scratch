Neural Network from Scratch

This project implements a neural network from scratch, utilizing ReLU, tanh, and sigmoid activation functions, trained on the MNIST handwritten digit dataset. The network's architecture and training process are implemented without relying on pre-built deep learning libraries, providing a foundational understanding of neural network fundamentals. The project aims to demonstrate the core concepts of forward and backpropagation, weight optimization, and the impact of different activation functions on network performance when classifying handwritten digits.
